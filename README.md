# Glue classification with facebook/bart and cardiffnlp/twitter-roberta-base

Для классификации датасетов SST-2, RTE и CoLA я решил использовать модели [facebook/bart-based](https://huggingface.co/facebook/bart-base) и [cardiffnlp/twitter-roberta-base](https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment) и сравнить качество, полученное с их помощью. Bart - SOTA архитектура для суммаризации текстов, появившаяся недавно и быстро получившая популярность, соответственно эмбеддинги из неё могут хорошо работать для классификации текстов. Roberta - более старая архитектура, но все ещё применяющаяся в задачах NLP. В данном случае она была обучена на постах из твиттера, что вполне может подходить для, например, предсказания логической связи в датасете RTE.

Результаты обучения с различными начальными learning_rate можно увидеть в таблице.
Классификация - датасеты CoLA и SST-2:
| Model name                                | Dataset | Loss                | Lr     | accuracy           | f1                 | precision          | recall             |
| ----------------------------------------- | ------- | ------------------- | ------ | ------------------ | ------------------ | ------------------ | ------------------ |
| cardiffnlp/twitter-roberta-base-sentiment | cola    | 0.52  | 2e-05  | 0.82 | 0.87 | 0.82 | 0.94 |
| cardiffnlp/twitter-roberta-base-sentiment | cola    | 0.40 | 5e-05  | 0.83 | 0.88  | 0.83 | 0.94 |
| cardiffnlp/twitter-roberta-base-sentiment | cola    | 0.57  | 0.0001 | 0.81 | 0.87  | 0.82 | 0.93 |
| facebook/bart-base                        | cola    | 0.47  | 2e-05  | 0.81 | 0.87 | 0.81 | 0.93 |
| facebook/bart-base                        | cola    | 0.54  | 5e-05  | 0.81 | 0.87 | 0.81 | 0.94 |
| facebook/bart-base                        | cola    | 0.57  | 0.0001 | 0.73 | 0.83  | 0.73 | 0.96 |
| cardiffnlp/twitter-roberta-base-sentiment | sst     | 0.22 | 2e-05  | 0.94 | 0.94 | 0.93 | 0.95 |
| cardiffnlp/twitter-roberta-base-sentiment | sst     | 0.22   | 5e-05  | 0.93 | 0.93 | 0.92 | 0.94 |
| cardiffnlp/twitter-roberta-base-sentiment | sst     | 0.55  | 0.0001 | 0.77 | 0.79 | 0.76 | 0.81 |
| facebook/bart-base                        | sst     | 0.22 | 2e-05  | 0.93 | 0.94 | 0.93 | 0.95 |
| facebook/bart-base                        | sst     | 0.28 | 5e-05  | 0.91 | 0.91 | 0.92 | 0.90 |
| facebook/bart-base                        | sst     | 0.27  | 0.0001 | 0.89 | 0.88 | 0.87 | 0.91 |

В датасете CoLA был сильный дисбаланс классов - на 6000 положительного класса приходилось 2500 экземпляров отрицательного класса. Для его устранения была использована аугментация к отрицательному классу на основе модели Word2Vec. Для получения векторов была взята модель fasttext, так как она умеет работать с неизвестными словами, работает относительно быстро и дает качественные векторные представления. Соответственно, удалось снизить дисбаланс классов. Для датасетов CoLA и SST-2 для снижения роли отличий в количестве классов также в качестве функции потерь использовалась взвешенная кросс-энтропия, дающая больший вес ошибкам меньшего класса. 

Изначально я хотел использовать попробовать различные также алгоритмы пулинга эмбеддингов токены в эмбеддинг текста для последующей классификации, как я делал в исследовании по биоинформатике, которым ранее занимался. Но ввиду ограниченности времени и того, что в данной постановке задачи модель очень легко переобучалась/недообучалась, я использовал встроенный в transformers класс AutoModelForSequenceClassification.

Из результатов классификации можно увидеть, что шаг обучения = 2e-5 является более оптимальным, чем шаги больше него, для обеих моделей (возможно, при меньшем шаге результат улучшится, но для тестирование были выбраны такие шаги как достаточно популярные). Также можно увидеть, что на датасете CoLA roberta обгоняет bart. Можно предположить, что это случилось из-за специфичности данных, на которых она была обучена - в сообщениях твиттера зачастую написаны не совсем грамотно => модель могла лучше научиться распознавать этот паттерн.

Полученные результаты согласуются, например, с [albert, обученным на CoLA](https://githubhelp.com/delirecs/text_classification_on_CoLA), и превосходят классический BERT на датасете SST-2, что можно увидеть из [данной статьи](https://www.assemblyai.com/blog/fine-tuning-transformers-for-nlp/)

В датасете RTE классы идеально сбалансированы, но сама по себе выборка очень небольшая. Для её увеличения были также использованы аугментации на основе Word2Vec, а также random_swap (в случае с, например, CoLA мы не могли его использовать, т.к. он повлиял бы на грамматическую корректность, но при предсказании логической связи он не должен сильно влиять). Это позволило частично защититься от переобучения.
